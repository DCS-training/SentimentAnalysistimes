{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of The Times Music Reviews\n",
    "## Part II: Sentiment Analysis\n",
    "*How have artforms been reported?  Is there a status hierarchy between them?  How has this changed over time?*\n",
    "\n",
    "* **Project:** What counts as culture?  Reporting and criticism in The Times 1785-2000\n",
    "* **Project Lead:** Dave O'Brien\n",
    "* **Developer:** Lucy Havens\n",
    "* **Funding:** from the Centre for Data, Culture & Society, University of Edinburgh\n",
    "\n",
    "Begun February 2021\n",
    "\n",
    "***\n",
    "\n",
    "First, import required programming libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For data loading\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "# # For data visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import altair as alt\n",
    "# import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentiment Analysis with NLTK's Naive Bayes Classifier\n",
    "*Code based on: https://www.nltk.org/howto/sentiment.html*\n",
    "\n",
    "Read the data and tokenize words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../TheTimes_DaveO/TheTimesTextFiles_1953-2000/\"\n",
    "articles = PlaintextCorpusReader(data_path, \".+\", encoding='utf-8')\n",
    "tokens = articles.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CORONATION', 'HONOURS', 'THE', 'THREE', 'NEW', 'PEERS', ':', 'ONE', 'ORDER', 'OF']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10]) # print the first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for fileid in articles.fileids():\n",
    "    sentences += [sent_tokenize(articles.raw(fileid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STOCK EXCHANGE DEALINGS The following list shows transactions marked on the Stock Exchange yesterday and also the latest markings during the week (date in brackets) of any security not marked yesterday.',\n",
       " 'Only one mark in any one security is recorded at any one price; the sequence of marking is not necessarily that in which the bargains were done.',\n",
       " 'Number of marks received in each section is shown in brackets after the name of the section concerned.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good!  Hopefully most of our data has been segmented into sentences this neatly...we'll find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Only', 'one', 'mark', 'in', 'any', 'one', 'security', 'is', 'recorded', 'at', 'any', 'one', 'price', ';', 'the', 'sequence', 'of', 'marking', 'is', 'not', 'necessarily', 'that', 'in', 'which', 'the', 'bargains', 'were', 'done', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_articles = []\n",
    "for article in sentences:\n",
    "    tokenized_sentences = []\n",
    "    for s in article:\n",
    "        tokens = word_tokenize(s)\n",
    "        tokenized_sentences += [tokens]\n",
    "    tokenized_articles += [tokenized_sentences]\n",
    "print(tokenized_articles[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's select a random subsets of the articles for a training set and a test set. We'll put 80% of the data in the training set and the remaining 20% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 571\n",
      "Training articles: 457\n",
      "Test articles: 114\n"
     ]
    }
   ],
   "source": [
    "total_articles = len(tokenized_articles)\n",
    "print(\"Total articles:\",total_articles)\n",
    "training_size = round(total_articles * 0.8)\n",
    "test_size = total_articles - training_size\n",
    "print(\"Training articles:\",training_size)\n",
    "print(\"Test articles:\", test_size)\n",
    "# random.sample(range(0, 1000), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 114\n"
     ]
    }
   ],
   "source": [
    "test_indeces = random.sample(range(0,total_articles),test_size)\n",
    "test_data = []\n",
    "for i in test_indeces:\n",
    "    article = tokenized_articles[i]\n",
    "    test_data += [article]\n",
    "\n",
    "print(\"Test data length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 457\n"
     ]
    }
   ],
   "source": [
    "indeces = range(0,total_articles)\n",
    "training_indeces = []\n",
    "for i in indeces:\n",
    "    if i not in test_indeces:\n",
    "        training_indeces += [i]\n",
    "\n",
    "training_data = []\n",
    "for i in training_indeces:\n",
    "    article = tokenized_articles[i]\n",
    "    training_data += [article]\n",
    "\n",
    "print(\"Training data length:\", len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis with NLTK's VADER\n",
    "*Code based on: https://www.nltk.org/howto/sentiment.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sentiment Analysis with TensorFlow's Keras\n",
    "https://www.google.com/search?channel=fs&client=ubuntu&q=tensorflow+keras+python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Analysis with TextBlob's Pattern Analyzer\n",
    "https://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
